# get clusters
clusters = daldridgetnt_pam$clustering
#Step 9: Let's try to get a nicer plot
# first we need to define a color palette
gbrew = brewer.pal(8, "Dark2")
# I like to use hsv encoding
gpal = rgb2hsv(col2rgb(gbrew))
# colors in hsv (hue, saturation, value, transparency)
gcols = rep("", k)
for (i in 1:k) {
gcols[i] = hsv(gpal[1,i], gpal[2,i], gpal[3,i], alpha=0.65)
}
#plot with frequencies
wcex = log10(rowSums(m1))
plot(mca$row$coord, type="n", xaxt="n", yaxt="n", xlab="", ylab="")
title("@daldridgetnt Correspondence Analysis of tweet words", cex.main=1)
for (i in 1:k)
{
tmp <- clusters == i
text(mca$row$coord[tmp,1], mca$row$coord[tmp,2],
labels=rownames(m1)[tmp], cex=wcex[tmp],
col=gcols[i])
}
#Step 10: For the ggploters, a similar graphic can be obtained like this
# create data frame
daldridgetnt_words_df = data.frame(
words = rownames(m1),
dim1 = daldridgetnt_ca$row$coord[,1],
dim2 = daldridgetnt_ca$row$coord[,2],
freq = rowSums(m1),
cluster = as.factor(clusters))
# plot
ggplot(daldridgetnt_words_df, aes(x=dim1, y=dim2, label=words)) +
geom_text(aes(size=freq, colour=cluster), alpha=0.7) +
scale_size_continuous(breaks=seq(20,80,by=1), range=c(3,8)) +
scale_colour_manual(values=brewer.pal(8, "Dark2")) +
labs(x="", y="") +
opts(title = "What does @daldridgetnt tweet about?",
plot.title = theme_text(size=12),
axis.ticks=theme_blank(),
legend.position = "none",
axis.text.x = theme_blank(),
axis.text.y = theme_blank()
)
#Step 3: Let's do some text cleaning
# remove retweet entities
daldridgetnt_clean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", daldridgetnt_txt)
# remove Atpeople
daldridgetnt_clean = gsub("@\\w+", "", daldridgetnt_clean)
# remove punctuation symbols
daldridgetnt_clean = gsub("[[:punct:]]", "", daldridgetnt_clean)
# remove numbers
daldridgetnt_clean = gsub("[[:digit:]]", "", daldridgetnt_clean)
# remove links
daldridgetnt_clean = gsub("http\\w+", "", daldridgetnt_clean)
#Step 4: Create Corpus, apply transformations, and get term-document matrix
# corpus
daldridgetnt_corpus = Corpus(VectorSource(daldridgetnt_clean))
# convert to lower case
daldridgetnt_corpus = tm_map(daldridgetnt_corpus, tolower)
# remove stoprwords
daldridgetnt_corpus = tm_map(daldridgetnt_corpus, removeWords, c(stopwords("english"), "daldridgetnt","david","alridge"))
# remove extra white-spaces
daldridgetnt_corpus = tm_map(daldridgetnt_corpus, stripWhitespace)
# term-document matrix
tdm = TermDocumentMatrix(daldridgetnt_corpus)
# convert as matrix
m = as.matrix(tdm)
#Step 5: We need to keep most frequent terms
For instance, let's keep those words that have a frequency > 90 percentile
# remove sparse terms (word frequency > 90% percentile)
wf = rowSums(m)
m1 = m[wf>quantile(wf,probs=0.9), ]
# remove columns with all zeros
m1 = m1[,colSums(m1)!=0]
# for convenience, every matrix entry must be binary (0 or 1)
m1[m1 > 1] = 1
#Step 6 Let's keep exploring by applying a cluster analysis
This will let us discover more about groups of words
# distance matrix with binary distance
m1dist = dist(m1, method="binary")
# for convenience, every matrix entry must be binary (0 or 1)
m1[m1 > 1] = 1
#Step 6 Lets keep exploring by applying a cluster analysis This will let us discover more about groups of words
# distance matrix with binary distance
m1dist = dist(m1, method="binary")
# cluster with ward method
clus1 = hclust(m1dist, method="ward")
plot(clus1, cex=0.7)
Correspondence Analysis (using package FactoMineR)
# correspondance analysis
daldridgetnt_ca = CA(m1, graph=FALSE)
#Step 7: For a better visualization, we can apply a Correspondence Analysis (using package FactoMineR)
# correspondance analysis
daldridgetnt_ca = CA(m1, graph=FALSE)
# default plot of words
plot(daldridgetnt_ca$row$coord, type="n", xaxt="n", yaxt="n", xlab="", ylab="")
text(daldridgetnt_ca$row$coord[,1], daldridgetnt_ca$row$coord[,2], labels=rownames(m1),
col=hsv(0,0,0.6,0.5))
title(main="@daldridgetnt Correspondence Analysis of tweet words", cex.main=1)
#Step 8: To improve the correspondance analysis plot, we can apply a clustering method
like k-means or partitioning around medoids (pam)
# partitioning around medoids iwth 6 clusters
k = 6
# pam clustering
daldridgetnt_pam = pam(daldridgetnt_ca$row$coord[,1:2], k)
# get clusters
clusters = daldridgetnt_pam$clustering
#Step 9: Let's try to get a nicer plot
# first we need to define a color palette
gbrew = brewer.pal(8, "Dark2")
# I like to use hsv encoding
gpal = rgb2hsv(col2rgb(gbrew))
# colors in hsv (hue, saturation, value, transparency)
gcols = rep("", k)
for (i in 1:k) {
gcols[i] = hsv(gpal[1,i], gpal[2,i], gpal[3,i], alpha=0.65)
}
#plot with frequencies
wcex = log10(rowSums(m1))
plot(mca$row$coord, type="n", xaxt="n", yaxt="n", xlab="", ylab="")
title("@daldridgetnt Correspondence Analysis of tweet words", cex.main=1)
for (i in 1:k)
{
tmp <- clusters == i
text(mca$row$coord[tmp,1], mca$row$coord[tmp,2],
labels=rownames(m1)[tmp], cex=wcex[tmp],
col=gcols[i])
}
#Step 10: For the ggploters, a similar graphic can be obtained like this
# create data frame
daldridgetnt_words_df = data.frame(
words = rownames(m1),
dim1 = daldridgetnt_ca$row$coord[,1],
dim2 = daldridgetnt_ca$row$coord[,2],
freq = rowSums(m1),
cluster = as.factor(clusters))
# plot
ggplot(daldridgetnt_words_df, aes(x=dim1, y=dim2, label=words)) +
geom_text(aes(size=freq, colour=cluster), alpha=0.7) +
scale_size_continuous(breaks=seq(20,80,by=1), range=c(3,8)) +
scale_colour_manual(values=brewer.pal(8, "Dark2")) +
labs(x="", y="") +
opts(title = "What does @daldridgetnt tweet about?",
plot.title = theme_text(size=12),
axis.ticks=theme_blank(),
legend.position = "none",
axis.text.x = theme_blank(),
axis.text.y = theme_blank()
)
#Step 9: Let's try to get a nicer plot
# first we need to define a color palette
gbrew = brewer.pal(8, "Dark2")
# I like to use hsv encoding
gpal = rgb2hsv(col2rgb(gbrew))
# colors in hsv (hue, saturation, value, transparency)
gcols = rep("", k)
for (i in 1:k) {
gcols[i] = hsv(gpal[1,i], gpal[2,i], gpal[3,i], alpha=0.65)
}
#plot with frequencies
wcex = log10(rowSums(m1))
plot(daldridgetnt_ca$row$coord, type="n", xaxt="n", yaxt="n", xlab="", ylab="")
title("@daldridgetnt Correspondence Analysis of tweet words", cex.main=1)
for (i in 1:k)
#plot with frequencies
wcex = log10(rowSums(m1))
plot(daldridgetnt_ca$row$coord, type="n", xaxt="n", yaxt="n", xlab="", ylab="")
title("@daldridgetnt Correspondence Analysis of tweet words", cex.main=1)
for (i in 1:k)
{
tmp <- clusters == i
text(mca$row$coord[tmp,1], mca$row$coord[tmp,2],
labels=rownames(m1)[tmp], cex=wcex[tmp],
col=gcols[i])
}
#plot with frequencies
wcex = log10(rowSums(m1))
plot(daldridgetnt_ca$row$coord, type="n", xaxt="n", yaxt="n", xlab="", ylab="")
title("@daldridgetnt Correspondence Analysis of tweet words", cex.main=1)
for (i in 1:k)
{
tmp <- clusters == i
text(daldridgetnt_ca$row$coord[tmp,1], daldridgetnt_ca$row$coord[tmp,2],
labels=rownames(m1)[tmp], cex=wcex[tmp],
col=gcols[i])
}
ls=rownames(m1)[tmp], cex=wcex[tmp],
col=gcols[i])
}
#Step 10: For the ggploters, a similar graphic can be obtained like this
# create data frame
daldridgetnt_words_df = data.frame(
words = rownames(m1),
dim1 = daldridgetnt_ca$row$coord[,1],
dim2 = daldridgetnt_ca$row$coord[,2],
freq = rowSums(m1),
cluster = as.factor(clusters))
# plot
ggplot(daldridgetnt_words_df, aes(x=dim1, y=dim2, label=words)) +
geom_text(aes(size=freq, colour=cluster), alpha=0.7) +
scale_size_continuous(breaks=seq(20,80,by=1), range=c(3,8)) +
scale_colour_manual(values=brewer.pal(8, "Dark2")) +
labs(x="", y="") +
opts(title = "What does @daldridgetnt tweet about?",
plot.title = theme_text(size=12),
axis.ticks=theme_blank(),
legend.position = "none",
axis.text.x = theme_blank(),
axis.text.y = theme_blank()
)
#Create a User Wordgraph
#1 Load the packages
# required packages
library(tm)
library(igraph)
library(ggplot2)
library(RColorBrewer
#2 Extract User Tweets
fp_tweets = userTimeline("Fake_Prokhorov", n=1000)
# extract text
fp_text = sapply(fp_tweets, function(x) x$getText())
#Step 3: Create corpus and term-document matrix
# create a corpus via VectorSource
fp_corpus = Corpus(VectorSource(fp_text))
# define list of transformations
fp_stopwords = unique(c(stopwords(), "fake_prokhorov", "via"))
# list of transformations
trans = list(weighting=weightTf, stopwords=fp_stopwords,
removePunctuation=TRUE,
tolower=TRUE,
minWordLength=4,
removeNumbers=TRUE)
# create a term-document matrix
fp_tdm = TermDocumentMatrix(fp_corpus, control=trans)
#Step 4: This is an optional step that might help us to get rid of some sparseterms
# Remove sparse terms from matrix
fp_clean = removeSparseTerms(fp_tdm, .995)
# as matrix
fp_clean = as.matrix(fp_clean)
#Step 5: Now we need to create the graph
# first create a word affiliations matrix
affi_matrix = fp_clean %*% t(fp_clean)
# then create an adjacency matrix with zeroes in its diagonal
adja_matrix = affi_matrix
diag(adja_matrix) = 0
# Create a graph
fp_graph = graph.adjacency(adja_matrix, weighted=TRUE)
#Step 6: In order to plot the graph we need to get the x, y coordinates
# coordinates for visualization
posi_matrix = layout.fruchterman.reingold(fp_graph, list(weightsA=E(fp_graph)$weight))
posi_matrix = cbind(V(fp_graph)$name, posi_matrix)
#Step 7: Join all the ingredients in a data frame
# create a data frame
fp_df = data.frame(posi_matrix, stringsAsFactors=FALSE)
names(fp_df) = c("word", "x", "y")
fp_df$x = as.numeric(fp_df$x)
fp_df$y = as.numeric(fp_df$y)
#Step 8: Make a first plot
# size effect
se = diag(affi_matrix) / max(diag(affi_matrix))
# plot
par(bg = "gray15")
with(fp_df, plot(x, y, type="n", xaxt="n", yaxt="n", xlab="", ylab="", bty="n"))
with(fp_df, text(x, y, labels=word, cex=log10(diag(affi_matrix)),
col=hsv(0.95, se, 1, alpha=se)))
#Step 9: To improve our graph, we can perform a k-means cluster analysis to find groups
# k-means with 7 clusters
words_km = kmeans(cbind(as.numeric(posi_matrix[,2]), as.numeric(posi_matrix[,3])), 7)
# add frequencies and clusters in a data frame
fp_df = transform(fp_df, freq=diag(affi_matrix), cluster=as.factor(words_km$cluster))
row.names(fp_df) = 1:nrow(fp_df)
#Step 10: Final plot
# graphic with ggplot
fp_words = ggplot(fp_df, aes(x=x, y=y)) +
geom_text(aes(size=freq, label=fp_df$word, alpha=.90, color=as.factor(cluster))) +
labs(x="", y="") +
scale_size_continuous(breaks = c(10,20,30,40,50,60,70,80,90), range = c(1,8)) +
scale_colour_manual(values=brewer.pal(8, "Dark2")) +
scale_x_continuous(breaks=c(min(fp_df$x), max(fp_df$x)), labels=c("","")) +
scale_y_continuous(breaks=c(min(fp_df$y), max(fp_df$y)), labels=c("","")) +
opts(panel.grid.major=theme_blank(),
legend.position="none",
panel.background=theme_rect(fill="gray10", colour="gray10"),
panel.grid.minor=theme_blank(),
axis.ticks=theme_blank(),
title = "Fake Prohokorov Tweets",
plot.title = theme_text(size=12))
# save the image in pdf format
ggsave(plot=fp_words, filename="Fake Prohkorov Wordgraph.pdf", height=10, width=10)
#Create a User Wordgraph
#1 Load the packages
# required packages
library(tm)
library(igraph)
library(ggplot2)
library(RColorBrewer)
#2 Extract User Tweets
fp_tweets = userTimeline("Fake_Prokhorov", n=1000)
# extract text
fp_text = sapply(fp_tweets, function(x) x$getText())
#Step 3: Create corpus and term-document matrix
# create a corpus via VectorSource
fp_corpus = Corpus(VectorSource(fp_text))
# define list of transformations
fp_stopwords = unique(c(stopwords(), "fake_prokhorov", "via"))
# list of transformations
trans = list(weighting=weightTf, stopwords=fp_stopwords,
removePunctuation=TRUE,
tolower=TRUE,
minWordLength=4,
removeNumbers=TRUE)
# create a term-document matrix
fp_tdm = TermDocumentMatrix(fp_corpus, control=trans)
#Step 4: This is an optional step that might help us to get rid of some sparseterms
# Remove sparse terms from matrix
fp_clean = removeSparseTerms(fp_tdm, .995)
# as matrix
fp_clean = as.matrix(fp_clean)
#Step 5: Now we need to create the graph
# first create a word affiliations matrix
affi_matrix = fp_clean %*% t(fp_clean)
# then create an adjacency matrix with zeroes in its diagonal
adja_matrix = affi_matrix
diag(adja_matrix) = 0
# Create a graph
fp_graph = graph.adjacency(adja_matrix, weighted=TRUE)
#Step 6: In order to plot the graph we need to get the x, y coordinates
# coordinates for visualization
posi_matrix = layout.fruchterman.reingold(fp_graph, list(weightsA=E(fp_graph)$weight))
posi_matrix = cbind(V(fp_graph)$name, posi_matrix)
#Step 7: Join all the ingredients in a data frame
# create a data frame
fp_df = data.frame(posi_matrix, stringsAsFactors=FALSE)
names(fp_df) = c("word", "x", "y")
fp_df$x = as.numeric(fp_df$x)
fp_df$y = as.numeric(fp_df$y)
#Step 8: Make a first plot
# size effect
se = diag(affi_matrix) / max(diag(affi_matrix))
# plot
par(bg = "gray15")
with(fp_df, plot(x, y, type="n", xaxt="n", yaxt="n", xlab="", ylab="", bty="n"))
with(fp_df, text(x, y, labels=word, cex=log10(diag(affi_matrix)),
col=hsv(0.95, se, 1, alpha=se)))
#Step 9: To improve our graph, we can perform a k-means cluster analysis to find groups
# k-means with 7 clusters
words_km = kmeans(cbind(as.numeric(posi_matrix[,2]), as.numeric(posi_matrix[,3])), 7)
# add frequencies and clusters in a data frame
fp_df = transform(fp_df, freq=diag(affi_matrix), cluster=as.factor(words_km$cluster))
row.names(fp_df) = 1:nrow(fp_df)
#Step 10: Final plot
# graphic with ggplot
fp_words = ggplot(fp_df, aes(x=x, y=y)) +
geom_text(aes(size=freq, label=fp_df$word, alpha=.90, color=as.factor(cluster))) +
labs(x="", y="") +
scale_size_continuous(breaks = c(10,20,30,40,50,60,70,80,90), range = c(1,8)) +
scale_colour_manual(values=brewer.pal(8, "Dark2")) +
scale_x_continuous(breaks=c(min(fp_df$x), max(fp_df$x)), labels=c("","")) +
scale_y_continuous(breaks=c(min(fp_df$y), max(fp_df$y)), labels=c("","")) +
opts(panel.grid.major=theme_blank(),
legend.position="none",
panel.background=theme_rect(fill="gray10", colour="gray10"),
panel.grid.minor=theme_blank(),
axis.ticks=theme_blank(),
title = "Fake Prohokorov Tweets",
plot.title = theme_text(size=12))
# save the image in pdf format
ggsave(plot=fp_words, filename="Fake Prohkorov Wordgraph.pdf", height=10, width=10)
View(fp_df)
getwd()
png(file="fake prohoky wordcloud.png", bg="black")
wordcloud(fp_df$words, freqDF$freq, scale=c(8, 0.25), min.freq=10, random.order=F, rot.per=0.2, colors=colorPalette)
dev.off()
library(wordcloud)
wordcloud(fp_df$words, freqDF$freq, scale=c(8, 0.25), min.freq=10, random.order=F, rot.per=0.2, colors=colorPalette)
dev.off()
colorPalette <- brewer.pal(8, "RdBu")
png(file="fake prohoky wordcloud.png", bg="black")
wordcloud(fp_df$words, freqDF$freq, scale=c(8, 0.25), min.freq=10, random.order=F, rot.per=0.2, colors=colorPalette)
dev.off()
png(file="fake prohoky wordcloud.png", bg="black")
wordcloud(fp_df$word, fp_df$freq, scale=c(8, 0.25), min.freq=10, random.order=F, rot.per=0.2, colors=colorPalette)
dev.off()
u = "http://www.draftexpress.com/agents/Agent-by-Client-NBA/"
tables = readHTMLTable(u)
??readHTMLTable
library("XML")
tables = readHTMLTable(u)
names(tables
)
Examples
# u = "http://en.wikipedia.org/wiki/World_population"
u = "http://www.draftexpress.com/agents/Agent-by-Client-NBA/"
tables = readHTMLTable(u)
names(tables)
tables
gsub("$w+", "", tables)
tables<-gsub("$w+", "", tables)
tables
tables = readHTMLTable(u)
names(tables)
tables
tables
head(tables)
tables<-gsub("$w+", "", tables)
tables
u = "http://en.wikipedia.org/wiki/List_of_countries_by_population"
tables = readHTMLTable(u)
names(tables)
tables[[2]]
u = "http://www.draftexpress.com/agents/Agent-by-Client-NBA/"
tables = readHTMLTable(u)
names(tables)
tables[[2]]
z = "http://en.wikipedia.org/wiki/List_of_countries_by_population"
tables = readHTMLTable(z)
names(tables)
tables[[2]]
tables2 = readHTMLTable(z)
names(tables2)
tables2[[2]]
#Agents
u = "http://www.draftexpress.com/agents/Agent-by-Client-NBA/"
tables = readHTMLTable(u)
names(tables)
tables[[2]]
tmp2 = tables2[[2]]
tmp2
tmp2)[-1]
names(tmp2)[-1]
names(tmp2)[-1])
names(tmp2)[-1]))
gsub("X", "", names(tmp2)[-1])
vals2 = cbind(year = as.integer(gsub("X", "", names(tmp2)[-1])),
matrix(as.integer(gsub(",", "", as.character(unlist(tmp2[-1])))),
ncol(tmp2)-1, byrow = TRUE, dimnames = list(NULL, as.character(tmp2[[1]]))))
vals2 = cbind(year = as.integer(gsub("X", "", names(tmp2)[-1])),
doc = htmlParse(z)
doc = htmlParse(z)
tableNodes = getNodeSet(doc, "//table")
tb = readHTMLTable(tableNodes[[2]])
tb
u = "http://www.draftexpress.com/agents/Agent-by-Client-NBA/"
tables = readHTMLTable(u)
names(tables)
tables[[2]]
# Print the table. Note that the values are all characters
# not numbers. Also the column names have a preceding X since
# R doesn't allow the variable names to start with digits.
tmp = tables[[2]]
# We can transform this to get the rows to be years and the columns
# to be population counts. We'll create a matrix.
vals = cbind(year = as.integer(gsub("X", "", names(tmp)[-1])),
matrix(as.integer(gsub(",", "", as.character(unlist(tmp[-1])))),
ncol(tmp)-1, byrow = TRUE, dimnames = list(NULL, as.character(tmp[[1]]))))
# Let's just read the second table directly by itself.
doc = htmlParse(u)
tableNodes = getNodeSet(doc, "//table")
doc = htmlParse(u)
tableNodes = getNodeSet(doc, "//table")
tb = readHTMLTable(tableNodes[[2]])
tb
tryAsInteger2 = function(node) {
val2 = xmlValue(node)
ans2 = as.integer(gsub(",", "", val))
if(is.na(ans))
val2
else
ans2
}
tryAsInteger2
tb2 = readHTMLTable(tableNodes2[[2]], elFun2 = tryAsInteger2)
tb2 = readHTMLTable2(tableNodes[[2]], elFun2 = tryAsInteger2,
colClasses2 = c("character", rep("integer", 9)))
tb = readHTMLTable(tableNodes[[2]])
tb\
tb
# Let's try to adapt the values on the fly.
# Let's try to adapt the values on the fly.
write.csv(tb,"Agents.csv")
tables = readHTMLTable(u)
names(tables)
t<-tables[[2]]
ls(t)
t
write.csv(t,"Agents2.csv")
fix(Opp)
2+2
install.packages("ggplot2")
install.packages("GGally")
library(rattle)
library("googlePublicData")
demo()
demo(dspl)
fix(mydspl)
View(simple)
install.packages(c("abcdeFBA", "abd", "ActuDistns", "ada", "adabag", "adehabitat", "adimpro", "agricolae", "alphashape3d", "Amelia", "anametrix", "anm", "ape", "asbio", "aspace", "astsa", "bams", "baseline", "basicspace", "BaSTA", "BatchExperiments", "BatchJobs", "BBmisc", "bda", "binGroup", "BioMark", "blockTools", "bnlearn", "BNPdensity", "boot", "BRugs", "caroline", "catnet", "cda", "CDM", "CDVine", "censReg", "CePa", "class", "cncaGUI", "CoClust", "coda", "compareGroups", "comparison", "complex.surv.dat.sim", "CompModSA", "conjoint", "copBasic", "copula", "CORElearn", "COUNT", "countrycode", "CPE", "CpGassoc", "cplm", "crblocks", "crmn", "crn", "cshapes", "cts", "DAAG", "DandEFA", "datamart", "dataview", "DDD", "deltaPlotR", "demography", "descr", "desire", "deSolve", "devtools", "dgof", "DiceView", "disclap", "Distance", "dlmap", "dlmodeler", "doRedis", "dpmixsim", "EcoTroph", "elasticnet", "emu", "EstCRM", "evd", "eVenn", "ExPosition", "extraBinomial", "extrafont", "ez", "FacPad", "FactoMineR", "faisalconjoint", "fanovaGraph", "faoutlier", "fast", "fastcox", "fda.usc", "fdrtool", "FIAR", "FisherEM", "flexclust", "FlexParamCurve", "forecast", "formatR", "fracprolif", "frailtypack", "FrF2", "FrF2.catlg128", "frmqa", "ftnonpar", "ftsa", "gamlss", "gamlss.dist", "gamm4", "gclus", "gcmr", "GeoLight", "GEOmap", "geoR", "GeoXp", "GEVcdn", "GGally", "GGEBiplotGUI", "GhcnDaily", "glmnet", "glpkAPI", "gMCP", "gmodels", "gnm", "googleVis", "goric", "GOSim", "GPvam", "gRain", "GrassmannOptim", "gRbase", "gridExtra", "gRim", "grpreg", "gstat", "gsubfn", "gumbel", "HAC", "harvestr", "hda", "heplots", "HIBAG", "HiDimDA", "HSROC", "HumMeth27QCReport", "hypred", "indicspecies", "influence.ME", "intamap", "intamapInteractive", "Interpol.T", "iRegression", "irr", "irtoys", "iSubpathwayMiner", "JM", "Johnson", "JOP", "kaps", "kequate", "KernSmooth", "KFAS", "kknn", "knitcitations", "knnGarden", "KoNLP", "Lambda4", "LaplacesDemon", "lcmm", "LEAPFrOG", "LearnEDA", "lessR", "lgcp", "linkcomm", "lmomco", "LogicReg", "lossDev", "magic", "MALDIquant", "mapReduce", "markdown", "MARSS", "MASS", "MasterBayes", "matrixcalc", "mbmdr", "mclust", "mcmc", "mcmcse", "MetabolAnalyze", "Metadata", "metaLik", "mgcv", "mice", "mirt", "misc3d", "missForest", "missMDA", "mlbench", "Mobilize", "monomvn", "MortalitySmooth", "mosaic", "moult", "movMF", "MPCI", "MplusAutomation", "mRm", "MSBVAR", "msm", "msProcess", "msr", "MultiPhen", "muma", "MuMIn", "mvmeta", "myepisodes", "NanoStringNorm", "NbClust", "NCBI2R", "ncg", "nnet", "nsRFA", "nullabor", "oce", "ODB", "Ohmage", "OneHandClapping", "OOmisc", "oposSOM", "optmatch", "orsk", "p3state.msm", "packS4", "PairedData", "pan", "parcor", "parser", "pcaL1", "pegas", "penalized", "pgirmess", "phangorn", "phcfM", "phytools", "PK", "planar", "plgp", "plotGoogleMaps", "plotrix", "plsdof", "polywog", "PowerTOST", "PredictABEL", "proj4", "psyphy", "ptw", "QCA", "qgraph", "qtlnet", "quantreg", "QuasiSeq", "R2jags", "R2OpenBUGS", "R2PPT", "R2SWF", "RAD", "rainbow", "RandomFields", "RankAggreg", "RaschSampler", "rattle", "rAverage", "rcdk", "Rcgmin", "RcmdrPlugin.DoE", "RcmdrPlugin.FactoMineR", "RcmdrPlugin.MPAStats", "Rcpp", "RcppBDT", "rCUR", "Rd2roxygen", "rda", "ref", "refund", "relax", "relSim", "RForcecom", "rgdal", "rgl", "rjson", "rJython", "RMendeley", "Rmisc", "RnavGraph", "RNetCDF", "robustfa", "RODBC", "rpart", "RPMG", "Rquake", "rredis", "RSEIS", "rsgcc", "rsm", "Rssa", "rtape", "RTOMO", "Rttf2pt1", "Runiversal", "RVAideMemoire", "SAScii", "sBF", "scam", "sdcTable", "seacarb", "season", "SEERaBomb", "seewave", "seg", "segmented", "selectiongain", "semGOF", "SemiParBIVProbit", "semTools", "SensoMineR", "SGP", "SGPdata", "sideChannelAttack", "SightabilityModel", "simboot", "simsem", "SixSigma", "SKAT", "skewtools", "snow", "SNPRelate", "sos4R", "spacetime", "sparkTable", "SPARQL", "sparr", "spatial", "SpatialEpi", "SpatialTools", "SpatialVx", "spatstat", "sperich", "spMC", "SPOT", "spTimer", "spuRs", "SQUAREM", "SRPM", "sspir", "StatMatch", "statmod", "SteinerNet", "stratification", "stringr", "subselect", "surveillance", "survey", "survSNP", "tabplot", "tabplotGTK", "taRifx", "TCC", "testthat", "TExPosition", "textir", "timeSeries", "TIMP", "tnet", "tolerance", "topologyGSA", "tree", "treebase", "treemap", "TreePar", "TreeSim", "trio", "truncnorm", "tseries", "TwoStepCLogit", "txtplot", "unmarked", "UScensus2000", "UScensus2000add", "UScensus2010", "VBLPCM", "vcdExtra", "verification", "VIM", "vines", "VisCov", "visualFields", "visualizationTools", "wavelets", "WDI", "websockets", "wfe", "WGCNA", "x12", "x12GUI", "XLConnect", "XLConnectJars", "xtermStyle", "yhat", "zic"))
)
install.packages(c("abcdeFBA", "abd", "ActuDistns", "ada", "adabag", "adehabitat", "adimpro", "agricolae", "alphashape3d", "Amelia"))
install.packages(c("anametrix", "anm", "ape", "asbio", "aspace", "astsa", "bams", "baseline", "basicspace", "BaSTA", "CoClust", "coda", "compareGroups", "comparison", "CompModSA", "conjoint", "copBasic", "copula", "CPE", "CpGassoc", "cplm", "crblocks", "crmn", "crn", "cshapes", "cts", "DAAG", "DandEFA", "datamart"))
z<-"D:\Users\Abresler\Google Drive"
setwd("D:/Users/Abresler/Google Drive")
getwd()
playbyplay.2006.2007.Regular.Season <- read.delim("D:/Users/Abresler/Google Drive/Cleansed Data for API/NBA/BasketballValue Dumps/2006-2007/2006-2007 Regular Season/playbyplay.2006.2007.Regular.Season.csv")
View(playbyplay.2006.2007.Regular.Season)
playbyplay.2006.2007.Regular.Season$Season<-"2006-2007"
playbyplay.2006.2007.Regular.Season$Season_Type<-"2006-2007"
View(playbyplay.2006.2007.Regular.Season)
playbyplay.2006.2007.Regular.Season$Season_Type<-"Regular"
setwd("D:/Users/Abresler/Google Drive/Cleansed Data for API/NBA/Season Data/Play by Play")
write.csv(playbyplay.2006.2007.Regular.Season,"playbyplay.2006.2007.Regular.Season.csv")
